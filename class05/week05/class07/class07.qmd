---
title: "Machine Learning 1"
author: "Muhammad Tariq"
format: pdf
toc: true


---
Today we explore the use of different data presentation tools on R like PCA


# First up kmean()

Demo of using kmean() function in base R. First make some data with a known structure.

```{r}
tmp <- c( rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

Now we have some made up data in 'x' lets see how kmeans() works with this data

```{r}
k <- kmeans(x,centers = 2, nstart = 20)
k
```


```{r}
plot(x, col=k$cluster)
points(k$centers,col="blue",pch=15)

```
# Add more clusters to the plots
```{r}
k <- kmeans(x,centers = 4, nstart = 20)
plot(x, col=k$cluster)
points(k$centers,col="blue",pch=15)
```

> **key-point**; K-means clustering is supper popular but can be miss-used. one big limitation is that it can impose a cliustyering pattern on your data even if clear natural grouping dont exist- i.e it does what you tell it to do in therms of 'center'. 


### Hierarchical Clustering 

the main function in "base" R for hierarchical clustering is called 'hclust'.

You can just pass our dataset as is into 'hclust()' you must give "distance matrix" as input. We can get this from the 'dist()' function in R.
```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

The results of 'hclust)' dont have a useful 'print()"
method but do have a speacial 'plot()' method. 


```{r}
plot(hc)
abline(h=8, col="red")
```

To get our main cluster assignmnet (membership vector) we need to "cut" the tree at the big goal posts


```{r}
grps <- cutree(hc, h=8)
grps
```

```{r}
table(grps)
```


```{r}
plot(x)
```

Hierarchical clustering is distinct in that the dendrogram (tree figure) can reveal the potential grouping in your data (unlike K-means)

## Principal Component Analysis (PCA)

PCA is a common and highly useful dimensionality 
reduction technique used in many fields - particularly bioinformatics. 

Here we eill analyze some data from UK on food consumption.

###data import

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)

head(x)
```

```{r}
rownames(x)<- x[,1]
x <- x[,-1]
head(x)
```

```{r}
x <- read.csv(url, row.names=1)
head(x)
```


```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))

```

one conventional plot that can be useful is called a "paris" plot. 

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```


### PCA TO THE RESUE

The main function in base R for PCA is called 'prcomp()'

```{r}
pca <- prcomp(t(x))
summary(pca)
```
### Interpretting PCA results

The 'prcomp() function returns a list object of our results with five attributes/components

```{r}
attributes(pca)
```

The two main "results" in here are 'pca$x' and 'pca$rotation'.
the first of these (pca$x') contains he scores of the data on the new PC axis - we use these to make our "PCA plot".

```{r}
pca$x
```

```{r}
library(ggplot2)

#Make a plot of pca$x with PC1 vs PC2
ggplot(pca$x) + 
  aes(PC1, PC2, label=rownames(pca$x)) +
  geom_point() +
  geom_text()
  
```
! PC1 enlightens the differnces in the rows, in how different the foods are consumed. 

The second major result is contained in the 'pca$rotation' object or component. Let's plot this to see what PCA is picking up...

```{r}
ggplot(pca$rotation) +
  aes(PC1, rownames(pca$rotation)) +
  geom_col()
```
!it shows how which food is eaten so differently. 

